"""Utility for initializing the database schema and creating a default admin.

This module can be executed as a script or imported and its
:func:`initialize_database` function called. It ensures that all tables
referenced by the SQLAlchemy models exist and optionally seeds an admin
account using the ``ADMIN_EMAIL`` and ``ADMIN_PASSWORD`` environment
variables. Environment variables are loaded from a ``.env`` file if
present via :func:`dotenv.load_dotenv`. When the ``POSTGRES_PASSWORD``
variable is defined (as it is in the Docker Compose deployment) the script
connects to the PostgreSQL host referenced by ``POSTGRES_HOST`` (defaulting
to ``"postgres"`` so Compose-based deployments continue to work) instead of
the legacy SQLite fallback provided by :class:`config.Config`.
"""

from __future__ import annotations

import os
from argparse import ArgumentParser
from pathlib import Path
import socket
from typing import Optional

from dotenv import load_dotenv

from sqlalchemy import create_engine, inspect, text
from sqlalchemy.engine import make_url
from sqlalchemy.exc import OperationalError

from app import create_app
from app.models import (
    db,
    User,
    Quote,
    ZipZone,
    CostZone,
    AirCostZone,
    Accessorial,
    HotshotRate,
    BeyondRate,
)
from scripts.import_air_rates import import_csvs as import_air_csvs
from scripts.import_hotshot_rates import import_csvs as import_hotshot_csvs
from quote.logic_hotshot import ZONE_X_PER_LB_RATE, ZONE_X_PER_MILE_RATE


def _ensure_resolvable_hostname(database_url: str) -> str:
    """Return a database URL whose hostname can be resolved locally.

    The helper checks whether the provided SQLAlchemy connection string uses
    the Docker Compose default host name ``"postgres"`` and verifies DNS
    resolution with :func:`socket.getaddrinfo`. When the lookup fails (common
    when running helper scripts outside of Docker) the host is rewritten to
    ``"localhost"`` so ``init_db`` can connect to a PostgreSQL server running
    on the same machine. Environment variables are updated to keep the rest of
    the script consistent with the adjusted URL.

    Args:
        database_url: Fully-qualified SQLAlchemy connection string generated by
            :func:`config.build_postgres_database_uri_from_env` or pulled from
            ``DATABASE_URL``.

    Returns:
        str: Either the original connection string when the host already
        resolves or a rewritten URL targeting ``localhost``.

    Side Effects:
        Sets ``POSTGRES_HOST`` and ``DATABASE_URL`` to ``"localhost"`` when a
        fallback is applied so subsequent configuration uses the reachable
        hostname.
    """

    url = make_url(database_url)
    host = url.host
    if host != "postgres":
        return database_url

    try:
        socket.getaddrinfo(host, url.port or 5432)
        return database_url
    except socket.gaierror:
        fallback_host = "localhost"
        fallback_url = url.set(host=fallback_host)
        current_host_env = os.getenv("POSTGRES_HOST")
        if current_host_env in {None, "", "postgres"}:
            os.environ["POSTGRES_HOST"] = fallback_host
        os.environ["DATABASE_URL"] = str(fallback_url)
        print(
            "🔁 Unable to resolve PostgreSQL hostname 'postgres'; retrying with 'localhost'."
        )
        return str(fallback_url)


def _seed_rate_tables(rates_dir: Path) -> None:
    """Reset and load all rate-related tables from CSV files.

    Args:
        rates_dir: Directory containing the CSV fixtures. The folder must
            include ``Hotshot_Rates.csv``, ``beyond_price.csv``,
            ``accessorial_cost.csv``, ``Zipcode_Zones.csv``,
            ``cost_zone_table.csv``, and ``air_cost_zone.csv``. Missing files
            are silently ignored because ``import_air_csvs`` and
            ``import_hotshot_csvs`` treat them as optional uploads.

    Side Effects:
        Empties all rate-related tables, loads rows from the provided CSV
        files, and ensures the special "zone X" fallback rate is present with
        the constants defined in :mod:`quote.logic_hotshot`.
    """

    if not rates_dir.exists():
        print(f"⚠️ Rate data directory not found at {rates_dir}. Rate tables are empty.")
        return

    ZipZone.query.delete()
    CostZone.query.delete()
    AirCostZone.query.delete()
    Accessorial.query.delete()
    HotshotRate.query.delete()
    BeyondRate.query.delete()
    db.session.commit()

    import_hotshot_csvs(rates_dir, db.session)
    import_air_csvs(rates_dir)

    # Ensure the fallback zone X rate exists with normalized pricing so hotshot
    # quotes can always fall back to the mileage-based logic defined in
    # ``quote.logic_hotshot``.
    zone_x = (
        HotshotRate.query.filter_by(zone="X").order_by(HotshotRate.miles.asc()).first()
    )
    if zone_x is None:
        zone_x = HotshotRate(
            miles=0,
            zone="X",
            per_lb=ZONE_X_PER_LB_RATE,
            per_mile=ZONE_X_PER_MILE_RATE,
            min_charge=0.0,
            weight_break=None,
            fuel_pct=0.0,
        )
        db.session.add(zone_x)
        print("✅ Added fallback hotshot zone X rate.")
    else:
        zone_x.per_lb = ZONE_X_PER_LB_RATE
        zone_x.per_mile = ZONE_X_PER_MILE_RATE
        zone_x.weight_break = None
        zone_x.min_charge = 0.0
        print("ℹ️ Normalized fallback hotshot zone X rate.")

    db.session.commit()

    counts = {
        "zip_zones": ZipZone.query.count(),
        "cost_zones": CostZone.query.count(),
        "air_cost_zones": AirCostZone.query.count(),
        "accessorials": Accessorial.query.count(),
        "hotshot_rates": HotshotRate.query.count(),
        "beyond_rates": BeyondRate.query.count(),
    }
    summary = ", ".join(f"{table}={count}" for table, count in counts.items())
    print(f"✅ Rate tables seeded from CSV files in {rates_dir}: {summary}.")


def initialize_database(data_dir: Optional[Path] = None) -> None:
    """Create database tables, seed rate data, patch schema, and seed an admin user.

    The function ensures all tables declared in ``app.models`` exist by
    invoking :func:`sqlalchemy.schema.MetaData.create_all` bound to
    ``db.engine`` and reports whether new tables were created by comparing
    the schema before and after creation. All rate tables are cleared and
    re-imported from CSV files via :func:`_seed_rate_tables` so the
    application starts with a consistent dataset. The seeding routine also
    normalizes the fallback "zone X" hotshot rate to the constants defined
    in :mod:`quote.logic_hotshot`. Legacy deployments missing newer columns
    on the ``quotes`` table are patched in-place. Environment variables are
    automatically loaded from a ``.env`` file. If ``ADMIN_EMAIL`` and
    ``ADMIN_PASSWORD`` are set, a default admin user is created if one does
    not already exist. When ``POSTGRES_PASSWORD`` is present the function
    builds the connection string with
    :func:`config.build_postgres_database_uri_from_env` so helper scripts
    running outside Docker honour overrides like ``POSTGRES_HOST`` without
    mutating ``DATABASE_URL``. When the Compose default hostname ``postgres``
    cannot be resolved (common when running outside of Docker) the connection
    string is rewritten to target ``localhost`` automatically. If the
    PostgreSQL server refuses the connection after resolution, the script
    falls back to the SQLite database configured on :class:`config.Config` so
    initialization continues on machines without PostgreSQL.

    Args:
        data_dir: Optional path to the directory containing rate CSV files.
            Defaults to the repository root or the path specified by the
            ``RATE_DATA_DIR`` environment variable.

    Raises:
        SystemExit: When the Compose default hostname ``postgres`` cannot be
            resolved even after attempting to fall back to ``localhost``. Set
            ``POSTGRES_HOST`` to a reachable hostname before rerunning the
            script.
    """

    load_dotenv()
    from config import Config, build_postgres_database_uri_from_env

    database_url = (
        os.getenv("DATABASE_URL")
        or build_postgres_database_uri_from_env()
        or Config.SQLALCHEMY_DATABASE_URI
    )
    database_url = _ensure_resolvable_hostname(database_url)

    # ``init_db`` historically fell back to SQLite when PostgreSQL was
    # unavailable. Retain that behaviour so local development machines without
    # a running database server can still initialize the schema.
    attempted_urls = {database_url}
    while True:
        pre_engine = create_engine(database_url)
        try:
            pre_tables = set(inspect(pre_engine).get_table_names())
            break
        except OperationalError as exc:
            url = make_url(database_url)
            host = url.host or ""
            original_message = str(getattr(exc, "orig", exc))
            lowered_message = original_message.lower()
            if (
                host == "postgres"
                and "could not translate host name" in lowered_message
            ):
                raise SystemExit(
                    "Unable to resolve the default PostgreSQL hostname 'postgres'. "
                    "Set POSTGRES_HOST to the reachable server (for example, "
                    "'localhost') before rerunning init_db.py."
                ) from exc

            # When the resolved host refuses the connection fall back to the
            # default SQLite database configured on ``Config``. This keeps the
            # developer experience aligned with the historic behaviour where the
            # script silently used SQLite when PostgreSQL was not available.
            fallback_url = Config.SQLALCHEMY_DATABASE_URI
            if (
                "connection refused" in lowered_message
                and fallback_url not in attempted_urls
            ):
                attempted_urls.add(fallback_url)
                database_url = fallback_url
                os.environ["DATABASE_URL"] = database_url
                print(
                    "⚠️ PostgreSQL connection refused; falling back to SQLite "
                    f"database at {database_url}."
                )
                continue

            raise
        finally:
            pre_engine.dispose()

    app = create_app()
    with app.app_context():
        db.metadata.create_all(bind=db.engine)

        # Legacy deployments created ``hotshot_rates.weight_break`` with a
        # ``NOT NULL`` constraint which prevents loading rows lacking a weight
        # break. Detect this older schema and recreate the table with the
        # correct nullable column so imports can succeed.
        inspector = inspect(db.engine)
        hotshot_cols = inspector.get_columns(HotshotRate.__tablename__)
        for col in hotshot_cols:
            if col["name"] == "weight_break" and not col["nullable"]:
                HotshotRate.__table__.drop(bind=db.engine)
                HotshotRate.__table__.create(bind=db.engine)
                print("🔧 Patched hotshot_rates.weight_break to allow NULL values.")
                # Refresh inspector to see new schema
                inspector = inspect(db.engine)
                break

        # Legacy schemas may also lack the optional ``per_mile`` column which is
        # required for distance pricing beyond 100 miles. Add it when missing so
        # newer code can safely query the table.
        hotshot_cols = inspector.get_columns(HotshotRate.__tablename__)
        column_names = {col["name"] for col in hotshot_cols}
        if "per_mile" not in column_names:
            db.session.execute(
                text("ALTER TABLE hotshot_rates ADD COLUMN per_mile FLOAT")
            )
            db.session.commit()
            print("🔧 Added hotshot_rates.per_mile column.")

        post_tables = set(inspect(db.engine).get_table_names())

        created_tables = sorted(post_tables - pre_tables)
        if created_tables:
            created = ", ".join(created_tables)
            print(f"✅ Created tables: {created}.")
        else:
            print("ℹ️ Tables already exist. Skipping creation.")

        # Determine CSV directory for seeding rate tables
        default_dir = Path(__file__).parent
        env_dir = os.getenv("RATE_DATA_DIR")
        rates_dir = data_dir or (Path(env_dir) if env_dir else default_dir)
        _seed_rate_tables(rates_dir)

        # Patch quotes table columns if missing (legacy schema support)
        inspector = inspect(db.engine)
        existing_columns = [
            col["name"] for col in inspector.get_columns(Quote.__tablename__)
        ]
        required_columns = {
            "actual_weight": "FLOAT",
            "dim_weight": "FLOAT",
            "pieces": "INTEGER",
            "length": "FLOAT",
            "width": "FLOAT",
            "height": "FLOAT",
        }
        for column, col_type in required_columns.items():
            if column not in existing_columns:
                db.session.execute(
                    text(
                        f"ALTER TABLE {Quote.__tablename__} ADD COLUMN {column} {col_type}"
                    )
                )
                print(f"🔧 Added missing column: {column}")
        db.session.commit()

        admin_email = os.getenv("ADMIN_EMAIL")
        admin_password = os.getenv("ADMIN_PASSWORD")

        if admin_email and admin_password:
            existing_admin = User.query.filter_by(email=admin_email).first()
            if not existing_admin:
                admin_user = User(email=admin_email, name="Admin", is_admin=True)
                admin_user.set_password(admin_password)
                db.session.add(admin_user)
                db.session.commit()
                print(f"✅ Default admin user created: {admin_email}")
            else:
                print(f"ℹ️ Admin user already exists: {admin_email}")
        else:
            print(
                "ℹ️ ADMIN_EMAIL or ADMIN_PASSWORD not set. "
                "Skipping admin user creation."
            )


if __name__ == "__main__":
    parser = ArgumentParser(description="Initialize database schema and seed rate data")
    parser.add_argument(
        "--data-dir",
        type=Path,
        help=(
            "Optional path to the rate CSV directory. Defaults to RATE_DATA_DIR "
            "or the repository root"
        ),
    )
    args = parser.parse_args()
    initialize_database(args.data_dir)
